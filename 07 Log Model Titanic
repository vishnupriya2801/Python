import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
get_ipython().magic('matplotlib inline')

train=pd.read_csv('titanic_train.csv')

import os
os.getcwd()
train.head()

#missing values in our data
sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')
sns.set_style('whitegrid')
sns.countplot(x='Survived',data=train)
sns.countplot(x='Survived',hue='Sex',data=train)
sns.countplot(x='Survived',hue='Pclass',data=train)
sns.distplot(train['Age'].dropna(),bins=30)
train.info()
sns.countplot(x='SibSp',data=train)

train['Fare'].hist(bins=50,figsize=(10,4))
sns.boxplot(x='Pclass',y='Age',data=train)

#imputation for missing values in age
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    
    if pd.isnull(Age):
        
        if Pclass ==1:
            return 37
        elif Pclass ==2:
            return 29
        else:
            return 24
        
    else:
        return Age


train['Age']=train[['Age','Pclass']].apply(impute_age,axis=1)

#nolonger any missing info for age
sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')

# I have removed the cabin column as it has a lot of missing values and do not add any value
train.drop('Cabin',axis=1,inplace=True)

#drop any furthermissing values
train.dropna(inplace=True)

#convert categorical values to dummy or indicator variables
pd.get_dummies(train['Sex'])
#female and male are perfectly collinear so we can drop on ecolumn
Sex=pd.get_dummies(train['Sex'],drop_first=True)
Sex
embark=pd.get_dummies(train['Embarked'],drop_first=True)
embark

#training and predictions
train=pd.concat([train,Sex,embark],axis=1)
train
train.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)
train.head()
train.info()
train.drop(['PassengerId'],axis=1,inplace=True)
train.head()
x=train.drop('Survived',axis=1)
y=train['Survived']

from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=101)

#model creation
from sklearn.linear_model import LogisticRegression
logmodel=LogisticRegression()
logmodel.fit(X_train,y_train)
predictions=logmodel.predict(X_test)

#confusion matrix and classification report
from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,predictions)

#do train and test sep as given
#cabin letter as afeature
#ticket info as a feature
#name mr/mrs may be used as a feature
#all these may increase the metrics and mode by feature engineering

